{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import json\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "ox.config(log_console=True)\n",
    "\n",
    "num_bins = 36\n",
    "threshold = 10 #meters\n",
    "\n",
    "input_path = 'data/sampled_graph_filepaths.csv'\n",
    "graphs_folder = 'G:\\\\Geoff\\\\osmnx\\\\data\\\\tracts\\\\graphml'\n",
    "bearings_folder = 'data/bearings'\n",
    "output_path = 'data/tracts_indicators.csv'\n",
    "cities_path = 'data/tracts_shapefile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/states_by_fips.json') as f:\n",
    "    fips_to_state = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepaths = pd.read_csv(input_path, header=None)[0].sort_values()\n",
    "len(filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_bearing(x):\n",
    "    return x + 180 if x < 180 else x - 180\n",
    "\n",
    "def get_unweighted_bearings(G, threshold):\n",
    "    # calculate edge bearings\n",
    "    # threshold lets you discard streets < some length from the bearings analysis\n",
    "    b = pd.Series([d['bearing'] for u, v, k, d in G.edges(keys=True, data=True) if d['length'] > threshold])\n",
    "    return pd.concat([b, b.map(reverse_bearing)]).reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_merge(n, bearings):\n",
    "    # make twice as many bins as desired, then merge them in pairs\n",
    "    # prevents bin-edge effects around common values like 0째 and 90째\n",
    "    n = n * 2\n",
    "    bins = np.arange(n + 1) * 360 / n\n",
    "    count, _ = np.histogram(bearings, bins=bins)\n",
    "    \n",
    "    # move the last bin to the front, so eg 0.01째 and 359.99째 will be binned together\n",
    "    count = np.roll(count, 1)\n",
    "    return count[::2] + count[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_orientation_entropy(data, n):\n",
    "    bin_counts = count_and_merge(n, data)\n",
    "    entropy = stats.entropy(bin_counts)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuity(G, edge_length_total):\n",
    "    \n",
    "    coords = np.array([[G.nodes[u]['y'], G.nodes[u]['x'], G.nodes[v]['y'], G.nodes[v]['x']] for u, v, k in G.edges(keys=True)])\n",
    "    df_coords = pd.DataFrame(coords, columns=['u_y', 'u_x', 'v_y', 'v_x'])\n",
    "\n",
    "    gc_distances = ox.great_circle_vec(lat1=df_coords['u_y'],\n",
    "                                    lng1=df_coords['u_x'],\n",
    "                                    lat2=df_coords['v_y'],\n",
    "                                    lng2=df_coords['v_x'])\n",
    "\n",
    "    gc_distances = gc_distances.fillna(value=0)\n",
    "    circuity_avg = edge_length_total / gc_distances.sum()\n",
    "    return circuity_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate length entropy and other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = {}\n",
    "\n",
    "for filepath in filepaths:\n",
    "    \n",
    "    div = filepath.rfind('/') + 1\n",
    "    folder = filepath[:div]\n",
    "    filename = filepath[div:]\n",
    "    geoid = filename.replace('.graphml', '')\n",
    "    \n",
    "    Gu = ox.get_undirected(ox.load_graphml(filename=filename, folder=folder))\n",
    "    lengths = pd.Series(nx.get_edge_attributes(Gu, 'length'))\n",
    "    \n",
    "    k_avg = 2 * len(Gu.edges()) / len(Gu.nodes())\n",
    "    n = len(Gu.nodes())\n",
    "    m = len(Gu.edges())\n",
    "    length_median = lengths.median()\n",
    "    length_mean = lengths.mean()\n",
    "    \n",
    "    # proportion of 4-way ints, dead-ends, and avg circuity\n",
    "    prop_4way = list(Gu.graph['streets_per_node'].values()).count(4) / len(Gu.nodes())\n",
    "    prop_deadend = list(Gu.graph['streets_per_node'].values()).count(1) / len(Gu.nodes())\n",
    "    circuity_avg = circuity(Gu, lengths.sum())\n",
    "    \n",
    "    # calculate length entropy\n",
    "    count, _ = np.histogram(lengths, num_bins)\n",
    "    length_entropy = stats.entropy(count)\n",
    "    count_log, _ = np.histogram(np.log(lengths+0.01), num_bins)\n",
    "    length_entropy_log = stats.entropy(count_log)\n",
    "    \n",
    "    # calculate orientation entropy\n",
    "    bearings = get_unweighted_bearings(ox.add_edge_bearings(Gu), threshold)\n",
    "    orientation_entropy = calculate_orientation_entropy(bearings.dropna(), num_bins)\n",
    "    \n",
    "    results[geoid] = {'k_avg'              : k_avg,\n",
    "                      'n'                  : n,\n",
    "                      'm'                  : m,\n",
    "                      'prop_4way'          : prop_4way,\n",
    "                      'prop_deadend'       : prop_deadend,\n",
    "                      'circuity_avg'       : circuity_avg,\n",
    "                      'length_median'      : length_median,\n",
    "                      'length_mean'        : length_mean,\n",
    "                      'length_entropy'     : length_entropy,\n",
    "                      'length_entropy_log' : length_entropy_log,\n",
    "                      'orientation_entropy': orientation_entropy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate orientation-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_entropy = np.log(num_bins)\n",
    "max_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bins = 4 #perfect grid\n",
    "perfect_grid = [1] * min_bins + [0] * (num_bins - min_bins)\n",
    "perfect_grid_entropy = stats.entropy(perfect_grid)\n",
    "perfect_grid_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orientation_order(eta, max_ent=max_entropy, min_ent=perfect_grid_entropy):\n",
    "    # normalize it as a value between perfect_grid_entropy and max_entropy\n",
    "    # then square it to approx linearize orientation_order's relationship with the\n",
    "    # share of total bins with equal non-zero probabilities\n",
    "    return 1 - ((eta - min_ent) / (max_ent - min_ent)) ** 2\n",
    "\n",
    "df['orientation_order'] = df['orientation_entropy'].map(orientation_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m'] = df['m'].astype(int)\n",
    "df['n'] = df['n'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['geoid', 'orientation_order', 'orientation_entropy', \n",
    "      'circuity_avg', 'k_avg', 'prop_deadend', 'prop_4way', 'm', 'n',\n",
    "      'length_median', 'length_mean', 'length_entropy', 'length_entropy_log']\n",
    "\n",
    "df = df.reset_index().rename(columns={'index':'geoid'}).reindex(columns=cols)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ox)",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
